{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SZSgeM200uG-","executionInfo":{"status":"ok","timestamp":1650309039619,"user_tz":-360,"elapsed":4276,"user":{"displayName":"Mehedi Mahmud","userId":"04554683503033399493"}},"outputId":"baf69da2-79b7-4141-eeeb-c5e739beac9c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"id":"SZSgeM200uG-"},{"cell_type":"markdown","id":"df5e9e34","metadata":{"id":"df5e9e34"},"source":["# Discriminator "]},{"cell_type":"markdown","id":"d0845fd7","metadata":{"id":"d0845fd7"},"source":["![image-2.png](attachment:image-2.png)"]},{"cell_type":"code","execution_count":2,"id":"62c54f1d","metadata":{"id":"62c54f1d","executionInfo":{"status":"ok","timestamp":1650309046390,"user_tz":-360,"elapsed":2769,"user":{"displayName":"Mehedi Mahmud","userId":"04554683503033399493"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","#Compact ReLu using instance normalization (NOT Batch norm)\n","class Block(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride):\n","        super().__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, 4, stride, 1, bias=True, padding_mode=\"reflect\"),\n","            #karnel size is always 4 in paper; padding = 1; padding_mode=reflect reduces artifacts\n","            nn.InstanceNorm2d(out_channels),\n","            nn.LeakyReLU(0.2, inplace=True), #adding Leaky ReLU activation function\n","        )\n","        \n","    def forward(self, x):\n","        return self.conv(x)\n","    \n","class Discriminator(nn.Module):\n","    def __init__(self, in_channels=3, features=[64, 128, 256, 512]): #in_channels=3 for RGB\n","        super().__init__()\n","        self.initial = nn.Sequential(\n","            nn.Conv2d(\n","                in_channels,\n","                features[0],\n","                kernel_size=4,\n","                stride=2,\n","                padding=1,\n","                padding_mode=\"reflect\"\n","            ),\n","            nn.LeakyReLU(0.2, inplace=True) #adding Leaky ReLU activation function\n","        )\n","        \n","        layers = []\n","        in_channels = features[0]\n","        for feature in features[1:]:\n","            layers.append(Block(in_channels, feature, stride=1 if feature==features[-1] else 2))\n","            in_channels = feature\n","        layers.append(nn.Conv2d(in_channels, 1, kernel_size = 4, stride=1, padding=1, padding_mode=\"reflect\"))\n","        self.model = nn.Sequential(*layers)\n","    \n","    def forward(self, x):\n","        x = self.initial(x)\n","        return torch.sigmoid(self.model(x)) # adding sigmoid at the end"]},{"cell_type":"markdown","id":"16b63752","metadata":{"id":"16b63752"},"source":["# Generator"]},{"cell_type":"markdown","id":"b7fed462","metadata":{"id":"b7fed462"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"code","execution_count":3,"id":"2bd95773","metadata":{"id":"2bd95773","executionInfo":{"status":"ok","timestamp":1650309049987,"user_tz":-360,"elapsed":558,"user":{"displayName":"Mehedi Mahmud","userId":"04554683503033399493"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class ConvBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, down=True, use_act=True, **kwargs):\n","        #down=True> meaning down sampling; kwargs> key word argument; use_act> activation function\n","        super().__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, padding_mode=\"reflect\", **kwargs)\n","            #Here, Key word arguments are kernel size, padding, stride\n","            if down\n","            else nn.ConvTranspose2d(in_channels, out_channels, **kwargs), #Used Decoder or Transpose Conv Layer\n","            nn.InstanceNorm2d(out_channels),\n","            nn.ReLU(inplace=True) if use_act else nn.Identity()\n","        )\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, channels):\n","        super().__init__()\n","        self.block = nn.Sequential(\n","            ConvBlock(channels, channels, kernel_size=3, padding=1), # Here stride = 1 by default\n","            ConvBlock(channels, channels, use_act=False, kernel_size=3, padding=1),\n","        )\n","\n","    def forward(self, x):\n","        return x + self.block(x)\n","\n","class Generator(nn.Module):\n","    def __init__(self, img_channels, num_features = 64, num_residuals=9):\n","        super().__init__()\n","        self.initial = nn.Sequential(\n","            nn.Conv2d(img_channels, num_features, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\"),\n","            nn.InstanceNorm2d(num_features),\n","            nn.ReLU(inplace=True),\n","        )\n","        self.down_blocks = nn.ModuleList(\n","            [\n","                ConvBlock(num_features, num_features*2, kernel_size=3, stride=2, padding=1),\n","                ConvBlock(num_features*2, num_features*4, kernel_size=3, stride=2, padding=1),\n","            ]\n","        )\n","        self.res_blocks = nn.Sequential(\n","            *[ResidualBlock(num_features*4) for _ in range(num_residuals)]\n","        )\n","        self.up_blocks = nn.ModuleList(\n","            [\n","                ConvBlock(num_features*4, num_features*2, down=False, kernel_size=3, stride=2, padding=1, output_padding=1),\n","                ConvBlock(num_features*2, num_features*1, down=False, kernel_size=3, stride=2, padding=1, output_padding=1),\n","            ]\n","        )\n","\n","        self.last = nn.Conv2d(num_features*1, img_channels, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\")\n","\n","    def forward(self, x):\n","        x = self.initial(x)\n","        for layer in self.down_blocks:\n","            x = layer(x)\n","        x = self.res_blocks(x)\n","        for layer in self.up_blocks:\n","            x = layer(x)\n","        return torch.tanh(self.last(x))"]},{"cell_type":"markdown","id":"b3f3d4ac","metadata":{"id":"b3f3d4ac"},"source":["# Dataset"]},{"cell_type":"code","execution_count":4,"id":"c25bfcf3","metadata":{"id":"c25bfcf3","executionInfo":{"status":"ok","timestamp":1650309053737,"user_tz":-360,"elapsed":807,"user":{"displayName":"Mehedi Mahmud","userId":"04554683503033399493"}}},"outputs":[],"source":["from PIL import Image\n","import os\n","from torch.utils.data import Dataset\n","import numpy as np\n","\n","class HorseZebraDataset(Dataset):\n","    def __init__(self, root_zebra, root_horse, transform=None):\n","        self.root_zebra = root_zebra\n","        self.root_horse = root_horse\n","        self.transform = transform\n","\n","        self.zebra_images = os.listdir(root_zebra)\n","        self.horse_images = os.listdir(root_horse)\n","        self.length_dataset = max(len(self.zebra_images), len(self.horse_images)) # 1000, 1500\n","        self.zebra_len = len(self.zebra_images)\n","        self.horse_len = len(self.horse_images)\n","\n","    def __len__(self):\n","        return self.length_dataset\n","\n","    def __getitem__(self, index):\n","        zebra_img = self.zebra_images[index % self.zebra_len] # No. of index might be bigger than zebra_images\n","        horse_img = self.horse_images[index % self.horse_len]\n","\n","        zebra_path = os.path.join(self.root_zebra, zebra_img)\n","        horse_path = os.path.join(self.root_horse, horse_img)\n","\n","        zebra_img = np.array(Image.open(zebra_path).convert(\"RGB\"))\n","        horse_img = np.array(Image.open(horse_path).convert(\"RGB\"))\n","\n","        if self.transform:\n","            augmentations = self.transform(image=zebra_img, image0=horse_img)\n","            zebra_img = augmentations[\"image\"]\n","            horse_img = augmentations[\"image0\"]\n","\n","        return zebra_img, horse_img"]},{"cell_type":"markdown","id":"1cd82123","metadata":{"id":"1cd82123"},"source":["# Config File"]},{"cell_type":"code","source":["#!pip install albumentations==0.4.6"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":779},"id":"t5bMLYWPFPLS","executionInfo":{"status":"ok","timestamp":1650309012844,"user_tz":-360,"elapsed":4834,"user":{"displayName":"Mehedi Mahmud","userId":"04554683503033399493"}},"outputId":"d173d374-efa9-4b0a-974d-dad97e8dd3b8"},"id":"t5bMLYWPFPLS","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting albumentations==0.4.6\n","  Downloading albumentations-0.4.6.tar.gz (117 kB)\n","\u001b[K     |████████████████████████████████| 117 kB 4.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.21.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.4.1)\n","Collecting imgaug>=0.4.0\n","  Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n","\u001b[K     |████████████████████████████████| 948 kB 30.3 MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (3.13)\n","Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.8.1.post1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n","Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.18.3)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.1.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2021.11.2)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.3.0)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.6.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.4.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (3.0.8)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.11.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (4.1.1)\n","Building wheels for collected packages: albumentations\n","  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for albumentations: filename=albumentations-0.4.6-py3-none-any.whl size=65174 sha256=2ded3369c2039d872b4266ee54211ce0e0f087367da9dd58ef6096ee96931c73\n","  Stored in directory: /root/.cache/pip/wheels/cf/34/0f/cb2a5f93561a181a4bcc84847ad6aaceea8b5a3127469616cc\n","Successfully built albumentations\n","Installing collected packages: imgaug, albumentations\n","  Attempting uninstall: imgaug\n","    Found existing installation: imgaug 0.2.9\n","    Uninstalling imgaug-0.2.9:\n","      Successfully uninstalled imgaug-0.2.9\n","  Attempting uninstall: albumentations\n","    Found existing installation: albumentations 0.1.12\n","    Uninstalling albumentations-0.1.12:\n","      Successfully uninstalled albumentations-0.1.12\n","Successfully installed albumentations-0.4.6 imgaug-0.4.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["albumentations","imgaug"]}}},"metadata":{}}]},{"cell_type":"code","execution_count":5,"id":"6b04f8da","metadata":{"id":"6b04f8da","executionInfo":{"status":"ok","timestamp":1650309059862,"user_tz":-360,"elapsed":1040,"user":{"displayName":"Mehedi Mahmud","userId":"04554683503033399493"}}},"outputs":[],"source":["import torch\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" #Code is running on CPU/GPU\n","TRAIN_DIR = \"/content/drive/MyDrive/Colab Notebooks/CVPR_Project/data/train\"\n","VAL_DIR = \"/content/drive/MyDrive/Colab Notebooks/CVPR_Project/data/val\"\n","BATCH_SIZE = 1\n","LEARNING_RATE = 1e-5\n","LAMBDA_IDENTITY = 0.0\n","LAMBDA_CYCLE = 10\n","NUM_WORKERS = 4\n","NUM_EPOCHS = 10\n","LOAD_MODEL = False\n","SAVE_MODEL = False\n","CHECKPOINT_GEN_H = \"genh.pth.tar\"\n","CHECKPOINT_GEN_Z = \"genz.pth.tar\"\n","CHECKPOINT_CRITIC_H = \"critich.pth.tar\"\n","CHECKPOINT_CRITIC_Z = \"criticz.pth.tar\"\n","\n","transforms = A.Compose(\n","    [\n","        A.Resize(width=256, height=256),\n","        A.HorizontalFlip(p=0.5),\n","        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255),\n","        ToTensorV2(),\n","     ],\n","    additional_targets={\"image0\": \"image\"},\n",")"]},{"cell_type":"markdown","id":"50514d47","metadata":{"id":"50514d47"},"source":["# Utils File"]},{"cell_type":"code","execution_count":6,"id":"5b5410a6","metadata":{"id":"5b5410a6","executionInfo":{"status":"ok","timestamp":1650309064348,"user_tz":-360,"elapsed":688,"user":{"displayName":"Mehedi Mahmud","userId":"04554683503033399493"}}},"outputs":[],"source":["import random, torch, os, numpy as np\n","import torch.nn as nn\n","#import config\n","import copy\n","\n","def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n","    print(\"=> Saving checkpoint\")\n","    checkpoint = {\n","        \"state_dict\": model.state_dict(),\n","        \"optimizer\": optimizer.state_dict(),\n","    }\n","    torch.save(checkpoint, filename)\n","\n","\n","def load_checkpoint(checkpoint_file, model, optimizer, lr):\n","    print(\"=> Loading checkpoint\")\n","    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n","\n","    # If we don't do this then it will just have learning rate of old checkpoint\n","    # and it will lead to many hours of debugging \\:\n","    for param_group in optimizer.param_groups:\n","        param_group[\"lr\"] = lr\n","\n","\n","def seed_everything(seed=42):\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False"]},{"cell_type":"markdown","id":"c38d5afa","metadata":{"id":"c38d5afa"},"source":["# Train Dataset"]},{"cell_type":"code","execution_count":8,"id":"e422ee8e","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":451},"id":"e422ee8e","executionInfo":{"status":"error","timestamp":1650309484647,"user_tz":-360,"elapsed":272281,"user":{"displayName":"Mehedi Mahmud","userId":"04554683503033399493"}},"outputId":"9708a3d0-91ce-4964-c570-0d6c68cbf7a5"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n","  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n","  1%|          | 8/1067 [04:31<9:58:35, 33.91s/it, H_fake=0.525, H_real=0.526]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-cdfdadf3fd5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-8-cdfdadf3fd5b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_H\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisc_Z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_Z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_H\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_disc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_scaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_scaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mSAVE_MODEL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-cdfdadf3fd5b>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(disc_H, disc_Z, gen_Z, gen_H, loader, opt_disc, opt_gen, l1, mse, d_scaler, g_scaler)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# identity loss (remove these for efficiency if you set lambda_identity=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0midentity_zebra\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_Z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzebra\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0midentity_horse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_H\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhorse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0midentity_zebra_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzebra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midentity_zebra\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-47ebd2db95f7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-47ebd2db95f7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-47ebd2db95f7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mResidualBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_mode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'zeros'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n\u001b[0m\u001b[1;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_pad\u001b[0;34m(input, pad, mode, value)\u001b[0m\n\u001b[1;32m   4187\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"reflect\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4189\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreflection_pad2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4190\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"replicate\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4191\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplication_pad2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch\n","#from dataset import HorseZebraDataset\n","import sys\n","#from utils import save_checkpoint, load_checkpoint\n","from torch.utils.data import DataLoader\n","import torch.nn as nn\n","import torch.optim as optim\n","#import config\n","from tqdm import tqdm\n","from torchvision.utils import save_image\n","#from discriminator_model import Discriminator\n","#from generator_model import Generator\n","\n","\n","#***************************************TRAIN FUNCTION********************************************\n","def train_fn(disc_H, disc_Z, gen_Z, gen_H, loader, opt_disc, opt_gen, l1, mse, d_scaler, g_scaler):\n","    H_reals = 0\n","    H_fakes = 0\n","    loop = tqdm(loader, leave=True)\n","\n","    for idx, (zebra, horse) in enumerate(loop):\n","        zebra = zebra.to(DEVICE)\n","        horse = horse.to(DEVICE)\n","        \n","        # Train Discriminators H and Z\n","        fake_horse = gen_H(zebra)\n","        D_H_real = disc_H(horse)\n","        D_H_fake = disc_H(fake_horse.detach())\n","        H_reals += D_H_real.mean().item()\n","        H_fakes += D_H_fake.mean().item()\n","        D_H_real_loss = mse(D_H_real, torch.ones_like(D_H_real))\n","        D_H_fake_loss = mse(D_H_fake, torch.zeros_like(D_H_fake))\n","        D_H_loss = D_H_real_loss + D_H_fake_loss #Loss Calculation for Hourse\n","\n","        fake_zebra = gen_Z(horse)\n","        D_Z_real = disc_Z(zebra)\n","        D_Z_fake = disc_Z(fake_zebra.detach())\n","        D_Z_real_loss = mse(D_Z_real, torch.ones_like(D_Z_real))\n","        D_Z_fake_loss = mse(D_Z_fake, torch.zeros_like(D_Z_fake))\n","        D_Z_loss = D_Z_real_loss + D_Z_fake_loss #Loss Calculation for Zebra\n","\n","        # put it togethor\n","        D_loss = (D_H_loss + D_Z_loss)/2\n","        \n","        \n","        # Train Discriminators H and Z\n","#         with torch.cuda.amp.autocast():\n","#             fake_horse = gen_H(zebra)\n","#             D_H_real = disc_H(horse)\n","#             D_H_fake = disc_H(fake_horse.detach())\n","#             H_reals += D_H_real.mean().item()\n","#             H_fakes += D_H_fake.mean().item()\n","#             D_H_real_loss = mse(D_H_real, torch.ones_like(D_H_real))\n","#             D_H_fake_loss = mse(D_H_fake, torch.zeros_like(D_H_fake))\n","#             D_H_loss = D_H_real_loss + D_H_fake_loss #Loss Calculation for Hourse\n","\n","#             fake_zebra = gen_Z(horse)\n","#             D_Z_real = disc_Z(zebra)\n","#             D_Z_fake = disc_Z(fake_zebra.detach())\n","#             D_Z_real_loss = mse(D_Z_real, torch.ones_like(D_Z_real))\n","#             D_Z_fake_loss = mse(D_Z_fake, torch.zeros_like(D_Z_fake))\n","#             D_Z_loss = D_Z_real_loss + D_Z_fake_loss #Loss Calculation for Zebra\n","\n","#             # put it togethor\n","#             D_loss = (D_H_loss + D_Z_loss)/2\n","\n","        opt_disc.zero_grad()\n","        d_scaler.scale(D_loss).backward()\n","        d_scaler.step(opt_disc)\n","        d_scaler.update()\n","\n","        # Train Generators H and Z\n","        # adversarial loss for both generators\n","        D_H_fake = disc_H(fake_horse)\n","        D_Z_fake = disc_Z(fake_zebra)\n","        loss_G_H = mse(D_H_fake, torch.ones_like(D_H_fake))\n","        loss_G_Z = mse(D_Z_fake, torch.ones_like(D_Z_fake))\n","\n","        # cycle loss\n","        cycle_zebra = gen_Z(fake_horse)\n","        cycle_horse = gen_H(fake_zebra)\n","        cycle_zebra_loss = l1(zebra, cycle_zebra)\n","        cycle_horse_loss = l1(horse, cycle_horse)\n","\n","        # identity loss (remove these for efficiency if you set lambda_identity=0)\n","        identity_zebra = gen_Z(zebra)\n","        identity_horse = gen_H(horse)\n","        identity_zebra_loss = l1(zebra, identity_zebra)\n","        identity_horse_loss = l1(horse, identity_horse)\n","\n","        # add all togethor\n","        G_loss = (\n","            loss_G_Z\n","            + loss_G_H\n","            + cycle_zebra_loss * LAMBDA_CYCLE\n","            + cycle_horse_loss * LAMBDA_CYCLE\n","            + identity_horse_loss * LAMBDA_IDENTITY\n","            + identity_zebra_loss * LAMBDA_IDENTITY\n","        )\n","        \n","        \n","        # Train Generators H and Z\n","#         with torch.cuda.amp.autocast():\n","#             # adversarial loss for both generators\n","#             D_H_fake = disc_H(fake_horse)\n","#             D_Z_fake = disc_Z(fake_zebra)\n","#             loss_G_H = mse(D_H_fake, torch.ones_like(D_H_fake))\n","#             loss_G_Z = mse(D_Z_fake, torch.ones_like(D_Z_fake))\n","\n","#             # cycle loss\n","#             cycle_zebra = gen_Z(fake_horse)\n","#             cycle_horse = gen_H(fake_zebra)\n","#             cycle_zebra_loss = l1(zebra, cycle_zebra)\n","#             cycle_horse_loss = l1(horse, cycle_horse)\n","\n","#             # identity loss (remove these for efficiency if you set lambda_identity=0)\n","#             identity_zebra = gen_Z(zebra)\n","#             identity_horse = gen_H(horse)\n","#             identity_zebra_loss = l1(zebra, identity_zebra)\n","#             identity_horse_loss = l1(horse, identity_horse)\n","\n","#             # add all togethor\n","#             G_loss = (\n","#                 loss_G_Z\n","#                 + loss_G_H\n","#                 + cycle_zebra_loss * LAMBDA_CYCLE\n","#                 + cycle_horse_loss * LAMBDA_CYCLE\n","#                 + identity_horse_loss * LAMBDA_IDENTITY\n","#                 + identity_zebra_loss * LAMBDA_IDENTITY\n","#             )\n","\n","        opt_gen.zero_grad()\n","        g_scaler.scale(G_loss).backward()\n","        g_scaler.step(opt_gen)\n","        g_scaler.update()\n","\n","        if idx % 200 == 0:\n","            save_image(fake_horse*0.5+0.5, f\"/content/drive/MyDrive/Colab Notebooks/CVPR_Project/saved_images/horse_{idx}.png\") #*0.5+0.5 to improve normalizarion\n","            save_image(fake_zebra*0.5+0.5, f\"/content/drive/MyDrive/Colab Notebooks/CVPR_Project/saved_images/zebra_{idx}.png\")\n","\n","        loop.set_postfix(H_real=H_reals/(idx+1), H_fake=H_fakes/(idx+1))\n","\n","\n","#********************MAIN FUNCTION*********************************************\n","def main():\n","    disc_H = Discriminator(in_channels=3).to(DEVICE) #Discriminator Horse\n","    disc_Z = Discriminator(in_channels=3).to(DEVICE) #Discriminator Zeebra\n","    gen_Z = Generator(img_channels=3, num_residuals=9).to(DEVICE) #Generate Zebra\n","    gen_H = Generator(img_channels=3, num_residuals=9).to(DEVICE) #Generate Horse\n","    opt_disc = optim.Adam(\n","        list(disc_H.parameters()) + list(disc_Z.parameters()),\n","        lr=LEARNING_RATE,\n","        betas=(0.5, 0.999),\n","    )\n","\n","    opt_gen = optim.Adam(\n","        list(gen_Z.parameters()) + list(gen_H.parameters()),\n","        lr=LEARNING_RATE,\n","        betas=(0.5, 0.999),\n","    )\n","\n","    L1 = nn.L1Loss() # Cycle Consistency Loss\n","    mse = nn.MSELoss() # Mean Square Error Loss\n","\n","    if LOAD_MODEL:\n","        load_checkpoint(\n","            CHECKPOINT_GEN_H, gen_H, opt_gen, LEARNING_RATE,\n","        )\n","        load_checkpoint(\n","            CHECKPOINT_GEN_Z, gen_Z, opt_gen, LEARNING_RATE,\n","        )\n","        load_checkpoint(\n","            CHECKPOINT_CRITIC_H, disc_H, opt_disc, LEARNING_RATE,\n","        )\n","        load_checkpoint(\n","            CHECKPOINT_CRITIC_Z, disc_Z, opt_disc, LEARNING_RATE,\n","        )\n","\n","    dataset = HorseZebraDataset(\n","        root_horse=TRAIN_DIR+\"/horses\", root_zebra=TRAIN_DIR+\"/zebras\", transform=transforms #Change Directory\n","    )\n","    val_dataset = HorseZebraDataset(\n","       root_horse=VAL_DIR+\"/horses\", root_zebra=VAL_DIR+\"/zebras\", transform=transforms\n","    )\n","    val_loader = DataLoader(\n","        val_dataset,\n","        batch_size=1,\n","        shuffle=False,\n","        pin_memory=True,\n","    )\n","    loader = DataLoader(\n","        dataset,\n","        batch_size=BATCH_SIZE,\n","        shuffle=True,\n","        num_workers=NUM_WORKERS,\n","        pin_memory=True\n","    )\n","    \n","    # Running in float16 [Delete ir for training in float32]\n","    g_scaler = torch.cuda.amp.GradScaler()\n","    d_scaler = torch.cuda.amp.GradScaler()\n","\n","    for epoch in range(NUM_EPOCHS):\n","        train_fn(disc_H, disc_Z, gen_Z, gen_H, loader, opt_disc, opt_gen, L1, mse, d_scaler, g_scaler)\n","\n","        if SAVE_MODEL:\n","            save_checkpoint(gen_H, opt_gen, filename=CHECKPOINT_GEN_H)\n","            save_checkpoint(gen_Z, opt_gen, filename=CHECKPOINT_GEN_Z)\n","            save_checkpoint(disc_H, opt_disc, filename=CHECKPOINT_CRITIC_H)\n","            save_checkpoint(disc_Z, opt_disc, filename=CHECKPOINT_CRITIC_Z)\n","\n","main()"]},{"cell_type":"code","execution_count":null,"id":"01bb28ab","metadata":{"id":"01bb28ab"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"CycleGAN-Draft1.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}